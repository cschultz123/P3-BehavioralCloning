# NVIDIA End-to-End Deep Learning

Developed neural network that maps raw pixel values to steering angle.

## Setup

**Steering Angle**: Turning radius (1/r)

Multiple Cameras: Using mutliple cameras can be used to ensure the car stays centered. The left and right are off center perspectives and can contribute to the steering angle command.

**TODO** Figure out how to map steering angle commands to left and right perspectives. This should be a function of the steering angle of the centered image.

The training set is composed of images and corresponding steering angle labels.

## Data Augmentation

It is important for the neural network to learn how to perform correctly in mistakes. To accomplish this data was used the provided the correct course of action when the car was off center.

The left and right cameras provide to shifts. Additional viewpoint transformations were applied to add additional shifts and rotations to the nearest camera (What do they mean by nearest camera?).

Assumption: Everything below the horizon is flat ground and everything above horizon are infinitely far away. (Figure out what this means?)

![Nvidia Training Pipeline](nvidia-training-pipeline-diagram.png)

![Nvidia Deployment Pipline](nvidia-deployment-pipline.png)

Consideration: Look into augmenting lighting conditions.

## Network Architecture

Loss: RMSE of steering command output by the network and the command of either the human driver, or the adjusted steering command for off-center and rorated images. 

Network Details:

* 9 layers
    - 1 normalization
        + Included in network to allow for it to be modified along with the network.
    - 5 convolutional (experimentally determined)
        + 3 stride = (2x2) kernel=(5x5) (why?)
        + 2 stride = (1x1) kernel=(3x3) (why?)
    - 3 fully connected
        + Used to serve as the controller. Generally convolutional layers are used for feature extraction. Unable to prove this though.
* YUV image (not RGB)

## Training Details

Labels: road type, weather condition, drivers activity (staying in lane, switching lanes, turning and so forth...)

The only data that was used was the data where the driver was staying in the lane. The data was also sampled at 10 FPS to avoid having images be too similar.

To remove bias towards driving in straight line more data involving curves was used.

![nvidia network diagram](nvidia_network.png)

## Augmentation

Artificial shifts and rotations are added to recover from poor position or orientation. The magnitude of the perturbations is chosen from a normal distribution with zero mean and standard deviation of twice that found with human drivers. 

**TODO**: How much did they find with human drivers?

## Simulation

The goal is to use a pre-recorded video to see what would appear if the CNN were used instead, to steer the vehicle.

Simulation Diagram:

![NVIDIA Simulator Diagram](nvidia_simulator_diagram.png)

The objective of the simulator is to see how the CNN performs given the same imagery as the car. 

1. Find the ground truth of the image orientation and position.
2. Update the image to correspond to the result of CNN steering commands.

Metrics evaluated in simulator: off center distance, yaw, and distance traveled.

Conditions: If the car moves more than one meter off center this triggers a virtual human intervention and the poition and orientation is reset to match the ground truth.

## Evaluation

The metric evaluation was the percent of autonomy per pre-recorded route. This was computed as one minus the number of interventions minus six seconds divided by the elapsed time multipled by 100. Six seconds was selected as the amount of time it would take a human to recover and resume autonomy.

$$autonomy = \left(1-\frac{(number_of_interventions)-(6\ seconds)}{elapsed_time}\right)*100$$




